---
title: "Data Modeling"
output: 
  rmarkdown::github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction

This section will focus on building a logistic regression model to predict customer churning based on my findings from the EDA section and measure its performance using the testing set. I will also attempt to improve the model's performance using methods such as bootstrapping. 

**Key Findings:**

- The initial logistic regression model had an accuracy of 79%, but a sensitivity of 46.9%. This means that over half of churning customers were incorrectly classified.
- By changing the probability threshold from 0.5 to 0.219, I was able to increase the sensitivity to 85.0% at the cost of a 10.1% decrease in accuracy. This translates to a 81.2% increase in the number of churning customers who are correctly classified.
- By using bootstrapping, I increased sensitivity to 76.1% at the cost of a 6.5% decrease in accuracy. 
- Depending on the cost of customer retention and customer loss, each model has its own advantages and disadvantages.
- The type of contract a customer has is the most important variable in predicting churning. Other important variables include their monthly charge, payment method, and whether or not they have certain services provided by the company.

### 1.1 Loading Packages

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Load required libraries
library(dplyr)
library(plyr)
library(reshape2)
library(caret)
library(cutpointr)

# Set seed for reproducible results
set.seed(101)
```

### 1.2 Importing Data

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Import data
load("data/train.Rdata")
load("data/test.Rdata")
```


## 2. Data Preparation

Before building the model, I need to prepare the dataset based on my findings from the EDA section. This includes handling missing values and feature engineering.

### 2.1 Data Cleaning

Since I imported the raw data again, I need to perform the same data cleaning I did in the EDA section to both the training and testing sets.

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Drop the first column, which contains customer ID
train <- subset(train, select = -c(customerID))
test <- subset(test, select = -c(customerID))

# Drop the missing values
train <- train[complete.cases(train), ]
test <- test[complete.cases(test), ]

# Change senior citizen to a factor variable
train$SeniorCitizen <- as.factor(mapvalues(train$SeniorCitizen, from=c("0","1"), to=c("No", "Yes")))
test$SeniorCitizen <- as.factor(mapvalues(test$SeniorCitizen, from=c("0","1"), to=c("No", "Yes")))
```

### 2.2 Feature Engineering

Next, I created 3 new features using the variables in the dataset:

1. **MonthlyContract** - whether or not a customer has a *month-to-month* contract
2. **HasService** - whether or not a customer has at least one of variables: online security, online backup, device protection, tech support
3. **ECheck** - whether or not a customer's payment method is *electronic check*

I verified the results by comparing their counts to the original variables or viewing sample rows of the data.

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Recode contract, payment method, and the offered services to create the new features
train$MonthlyContract <- as.factor(ifelse(train$Contract == "Month-to-month", "Yes", "No"))

train$HasService <- as.factor(ifelse(train$OnlineSecurity == "Yes", "Yes",
                              ifelse(train$OnlineBackup == "Yes", "Yes",
                              ifelse(train$DeviceProtection == "Yes", "Yes",
                              ifelse(train$TechSupport == "Yes", "Yes", "No")))))

train$ECheck <- as.factor(ifelse(train$PaymentMethod == "Electronic check", "Yes", "No"))

# Verify the new features by comaparing counts
summary(subset(train, select = c("MonthlyContract", "Contract")))
summary(subset(train, select = c("ECheck", "PaymentMethod")))

# Verify the new feature by viewing sample rows of the dataset
sample_n(subset(train, select = c("HasService", "OnlineSecurity", "OnlineBackup", "DeviceProtection", "TechSupport")), 5)
```

Once I verified the changes, I created the features in the testing set.

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Create features in the testing set
test$MonthlyContract <- as.factor(ifelse(test$Contract == "Month-to-month", "Yes", "No"))

test$HasService <- as.factor(ifelse(test$OnlineSecurity == "Yes", "Yes",
                              ifelse(test$OnlineBackup == "Yes", "Yes",
                              ifelse(test$DeviceProtection == "Yes", "Yes",
                              ifelse(test$TechSupport == "Yes", "Yes", "No")))))

test$ECheck <- as.factor(ifelse(test$PaymentMethod == "Electronic check", "Yes", "No"))
```

### 2.3 Validation Set

Finally, I split half the testing set to create a validation set. The validation set will be used to measure initial model performance while the testing set will be reserved to measure performance after validation.

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Split the test set in half to create a test set and a validation set
sample <- sample(nrow(test), nrow(test) * 0.5, replace = FALSE)
val.set <- test[-sample, ]
test <- test[sample, ]
```

## 3. Logistic Regression Model

The logistic regression model involves modeling the log-odds ratio of a customer churning. For a more detailed description of the method and why I used it, see my blog post.

### 3.1 Building the Model

I fit a logistic regression model on **churning** and the 5 variables I determined to be significant from the EDA section.

The predictors are:

1. Monthly Contract
2. Monthly Charges
3. Dependents
4. Electronic Check
5. Has Service

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Fit logistic regression model
log.fit <- glm(Churn ~ MonthlyContract + MonthlyCharges + Dependents + ECheck + HasService, data = train, family = binomial)

# View summary of the logistic regression model
summary(log.fit)


```

**Summary of Findings:**

- The estimated coefficient for **monthly contract** is 2.002, which means the log-odds ratio of churning is 2.002 greater if a customer has a month-to-month contract.
- The estimated coefficient for **monthly charges** is 0.023, which means for every $1 increase in the monthly subscription cost, the log-odds ratio of churning increases by 0.023.
- The estimated coefficient for **dependents** is -0.213, which means the log-odds ratio of churning is 0.213 less if a customer has dependents.
- The estimated coefficient for **electronic check** is 0.655, which means the log-odds ratio of churning is 0.655 greater is a customer uses electronic checks as their payment method
- The estimated coefficient for **has service** is -0.720, which means the log-odds ratio of churning is 0.720 less if a customer has at least one of the following: online security, online backup, device protection, or tech support.
- All estimated coefficients are significant except for **dependents**.


### 3.2 Calculating Model Performance

To calculate model performance, I defined a function that will fit the logistic regression model on the validation and testing sets and obtain the confusion matrix. This confusion matrix will be used to calculate the performance metrics such as accuracy, sensitivity, and specificity. 

```{r echo = TRUE, warning = FALSE, message=FALSE}
confusion.matrix <- function(model, k, data) {
  
  # Obtain fitted probabilities from the test/validation data
  glm.prob <- predict(model, type = "response", newdata = data)
  
  # Obtain vector of predictions using a k-value threshold
  glm.pred <- rep("No", nrow(data))
  glm.pred[glm.prob >= k] = "Yes"
  
  # Obtain confusion matrix and metrics
  cmat <- table(glm.pred, data$Churn)
  metrics <- confusionMatrix(cmat, positive = "Yes")
  
  # Display class counts
  print(summary(data$Churn))
  
  # Display confusion matrix
  print(metrics$table)
  
  # Print out accuracy, sensitivity, and specificity measures
  cat("\nAccuracy: ", round(metrics$overall["Accuracy"], 3))
  cat("\nSensitivity: ", round(metrics$byClass["Sensitivity"], 3))
  cat("\nSpecificity: ", round(metrics$byClass["Specificity"], 3))
  
  return(metrics)
}
```

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Obtain the confusion matrix of the model with a 0.5 probability threshold
log.matrix <- confusion.matrix(log.fit, 0.5, val.set)
```

Although the model has an accuracy of 79%, the sensitivity is only 46.9%. This means that out of the 226 churning customers in the validation set, more than half, (120 customers), have been incorrectly classified as not churning. This can be costly to the company if they fail to target customers who are likely to leave. Thus, adjustments to the model must be made. 

## 4. Improving the Model

### 4.1 Determining the Optimal Cutoff Point

One way to improve the model is to change the probability threshold, or cutoff point, when classifying customers as churning or not. To identify the optimal cutoff point, I plotted a ROC curve of the model using the predicted probabilities, which graphs the true positive rate (Sensitivity) against the false positive rate (1 - Specificity).

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Store predicted probabilities in the dataframe
glm.prob <- predict(log.fit, type = "response", newdata = val.set)
val.set$prob <- glm.prob

# Using the predicted probabilities, construct a ROC Curve
cp <- cutpointr(val.set, prob, Churn, pos_class = "Yes", neg_class = "No", direction = ">=", method = maximize_metric, metric = sum_sens_spec)

# Summarize the results of the ROC curve
summary(cp)
```

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Visualize the ROC curve
plot(cp)
```

The optimal cutoff value was determined to be 0.219. This means that if the model's predicted probability of churning is 0.219 or more, the customer will be classified as churning. 

The cutoff point can be located in the upper left hand corner of the ROC curve, where it attempts to maximize sensitivity and specificity. The plots above show the distribution of predicted probabilities under each class. As shown by the plot, the majority of churning customers were correctly classified. 

By changing the optimal cutoff value, the sensitivity increased to 0.850 at the cost of a lower specificity at 0.641. Depending on the costs of customer retention strategies and the cost of losing a customer, the trade off between sensitivity and specificity could be worth it. 

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Obtain the optimal cutoff value from the ROC curve
k <- cp$optimal_cutpoint

# Construct a new confusion matrix with the optimal cutoff
log.matrix2 <- confusion.matrix(log.fit, k, val.set)
```

192 out of 226 churning customers have been correctly classified as churning, which is an increase of 72 customers when using a 0.5 cutoff. However, 277 out of the 772 non-churning customers have been incorrectly classified as churning, which is an increase of 187 customers. 

### 4.2 Bootstrapping

Another way of improving model performance is bootstrapping. Bootstrapping involves sampling from the available data with replacement (i.e., oversampling). I will sample enough observations from each class of customers until I have an equal number of customers who churned and who did not churn.

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Split the training data by churning
train.churn <-  subset(train, Churn == "Yes")
train.not.churn <-  subset(train, Churn == "No")

# Sample from each set by the number of customers in the opposite set
churn.sample <- sample(nrow(train.churn), nrow(train.not.churn), replace = TRUE)
not.churn.sample <- sample(nrow(train.not.churn), nrow(train.churn), replace = TRUE)

# Add the bootstrapped samples to the training set
train.boot <- rbind(train, train.churn[churn.sample,], train.not.churn[not.churn.sample,])

# Verify that the prevalence of churning is 50%
summary(train.boot$Churn)
```

Once I have my bootstrapped samples, I will fit a logistic regression model on the data.

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Fit logistic regression model on the bootstrapped dataset
log.fit.boot <- glm(Churn ~ MonthlyContract + MonthlyCharges + Dependents + ECheck + HasService, data = train.boot, family = binomial)

# View summary of the resulting logistic regression model
summary(log.fit.boot)
```

The resulting coefficients are interpreted the same way as the initial model. In fact, many of the coefficients are similar to the initial model. In this model, however, the **dependents** coefficient is significant.

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Calculate the confusion matrix
boot.matrix <- confusion.matrix(log.fit.boot, 0.5, val.set)
```

The bootstrapped model has an accuracy of 72.5%, a sensitivity of 76.1%, and a specificity of 71.5%. Compared to the first model, the sensitivity is higher, but not as high as the sensitivity from changing the cutoff value. 

### 4.3 Plotting Model Performance

To measure the final model performance of the models, I will now use the testing set, which acts as completely unseen data for the models. This will be a good representation on how the models will perform on new data. 

```{r echo = TRUE, warning = FALSE, message=FALSE}
log.matrix.test <- confusion.matrix(log.fit, k, test)
```

```{r echo = TRUE, warning = FALSE, message=FALSE}
boot.matrix.test <- confusion.matrix(log.fit.boot, 0.5, test)
```

To easily compare the performance of all the models, I created a function that will store the metrics into a dataframe.

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Define function for extracting desired performance metric measures into a dataframe
add.performance.metrics <- function(metrics, model) {
  accuracy <- round(metrics$overall["Accuracy"], 3)
  sensitivity <- round(metrics$byClass["Sensitivity"], 3)
  specificity <- round(metrics$byClass["Specificity"], 3)
  balanced.accuracy <- round(metrics$byClass["Balanced Accuracy"], 3)
  
  return(data.frame(model, accuracy, sensitivity, specificity, balanced.accuracy, row.names = NULL))
}

# Create empty dataframe of performance metrics
metrics.df <- data.frame(matrix(ncol = 5, nrow = 0), row.names = NULL)

# Rename columns
col <- c("model", "accuracy", "sensitivity", "specificity", "balanced_accuracy")
colnames(metrics.df) <- col

# Create a list of the calculated confusion matrices
matrices <- list(log.matrix, log.matrix2, log.matrix.test, boot.matrix, boot.matrix.test)

# Create a vector of model names
names <- c("1. Base Model", "2. Optimal Cutoff", "3. Optimal Cutoff (Test)", "4. Bootstrapped Model","5. Bootstrapped Model (Test)")

# Iterate through each confusion matrix and use function to obtain performance metrics
for (i in 1:length(matrices)) {
  df <- add.performance.metrics(matrices[[i]], names[i])
  metrics.df <- rbind(metrics.df, df)
}

# View performance metrics dataframe
metrics.df
```

Then, I plotted the performance metrics of the models below.

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Expand dataframe to a long format
metrics.long <- melt(metrics.df, id.vars = "model", variable.name = "metrics")

# Plot metrics
ggplot(metrics.long, aes(model, value, group = metrics)) + 
  geom_point(aes(color = metrics), size = 2) + 
  geom_line(aes(color = metrics), size = 1) + 
  xlab("Model") + 
  ylab("Value") + 
  ggtitle("Model Performance") +
  theme_light() +
  theme(axis.text.x = element_text(angle = 15)) 
```

For each model, the results from the testing set is slightly lower than the results from the validation set. The **balanced accuracy**, which is the average of sensitivity and specificity, is equal for both models at 70.4%. 

Since the balanced accuracy of both models is equal, I decided to select the bootstrapped as the final model due to its higher accuracy (69.6% vs. 65.9%). 

## 5. Discussion

### 5.1 Feature Importance

Feature importance measures how influential a predictor is in predicting churning. One way of measuring influence is by looking at the coefficients of the model.

```{r echo = TRUE, warning = FALSE, message=FALSE}
# Obtain odds ratio for the coefficients
exp(log.fit.boot$coefficients)
```

Assuming all other remaining variables are constant:

- Customers with a month-to-month contract have a **622% increase** in the odds of churning.
- Customers with a dependent have a **28.8% decrease** in the odds of churning.
- Customers who use electronic checks have a **79.2% increase** in the odds.
- Customers that have one of those services have a **49.4% decrease** in the odds of churning.
- For every $1 increase in monthly charge, the odds of churning **increases by 2.5%**.

All predictors appear to have a significant influence in churning. Based on these numbers, **monthly contract** has the most influence in the odds of churning while **dependents** has the least influence. Thus, the Telco company can prioritize customers who have a month-to-month contract. 

### 5.2 Model Comparison

As previously stated, both models have an equal balanced accuracy of 70.4%. I decided to select the bootstrapped model as the final model due to its higher accuracy (69.6% vs. 65.9%). However, the regular logistic regression model (with the optimal cutoff point) could be the better model due to its higher sensitivity (80.7% s 72.3%).

For example, if the loss of a customer is more costly than the cost of any implemented customer retention strategies, then this model will be a better option since it ensures that customers who are more likely to churn are identified.

On the other hand, if the cost is about the same, then the bootstrapped model will be a better option since it ensures that as many customers are correctly classified regardless of whether or not they are more likely to churn or not. 

### 5.3 Other Ways to Improve Performance

Changing the probability threshold and bootstrapping both led to improved models. However, there are other options I could implement to improve the model.

First, I only selected 5 variables from the 19 potential predictors in the dataset, so it might be worth revisiting the other variables. Also, there were some predictors chosen that were highly correlated to another. For example, **internet service** is highly correlated with **monthly charges**, so I could include that in the model instead of monthly charges. The same is also true for **tenure** and **monthly contract**.

Finally, there are other classification methods I can use such as random forest to predict churning that might result in a better performance. I could determine which methods will be appropriate to use and perform any necessary hyperparameter tuning. 

## 6. Conclusion

In this project, I managed to develop a logistic regression model that predicts customers churning. I was able to increase the performance of the model by changing the probability threshold of classification and by using bootstrapping. Depending on the cost of implementing customer retention strategies and the cost of losing a customer, each model has its own advantages and disadvantages. Finally, by including only five predictors, the model is easily interpretable with actionable insights that can be derived from it, such as the type of customers to target and prioritize. 
